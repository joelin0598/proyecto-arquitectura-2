# version: "3.9"

name: monitoreo-stack

# ----------------------- Postgres, PgAdmin y exporter-----------------------
services:
  postgres:
    image: postgres:16-alpine
    container_name: db-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-appuser}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-appsecret}
      POSTGRES_DB: ${POSTGRES_DB:-alertsdb}
      POSTGRES_HOST_AUTH_METHOD: md5
      POSTGRES_INITDB_ARGS: --auth=md5
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [labnet]

  pgadmin:
    image: dpage/pgadmin4:8
    container_name: ui-pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-admin@example.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD:-admin123}
    ports:
      - "5050:80"
    depends_on:
      postgres:
        condition: service_healthy
    networks: [labnet]

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter
    container_name: postgres-exporter
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: "postgresql://appuser:appsecret@db-postgres:5432/alertsdb?sslmode=disable"
    ports:
      - "9187:9187"
    networks: [labnet]

  # ----------------------- Kafka y Kafka UI -----------------------

  kafka:
    image: confluentinc/cp-kafka:7.7.0
    container_name: kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      # --- KRaft single-node ---
      CLUSTER_ID: "MkU3OEVBNTcwNTJENDM2Qk"
      KAFKA_NODE_ID: "1"
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_LISTENERS: "PLAINTEXT://:9092,CONTROLLER://:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      # --- ajustes Ãºtiles ---
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: "24"
      KAFKA_NUM_PARTITIONS: "3"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"
    volumes:
      - kafkadata:/var/lib/kafka/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "/usr/bin/kafka-topics --bootstrap-server localhost:9092 --list >/dev/null 2>&1",
        ]
      interval: 10s
      timeout: 5s
      retries: 20
    networks: [labnet]

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: unless-stopped
    ports:
      - "8085:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_READONLY: "false"
    depends_on:
      - kafka
    networks: [labnet]

  # ----------------------- Redis y RedisInsight -----------------------

  redis:
    image: redis:7-alpine
    container_name: cache-redis
    restart: unless-stopped
    command: ["redis-server", "--appendonly", "yes", "--save", "60", "1000"]
    ports:
      - "6379:6379"
    volumes:
      - redisdata:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [labnet]

  redisinsight:
    image: redis/redisinsight:latest
    container_name: ui-redisinsight
    restart: unless-stopped
    ports:
      - "5540:5540"
    volumes:
      - redisinsight:/data
    depends_on:
      redis:
        condition: service_started
    networks: [labnet]

  # ----------------------- Monitoring Stack -----------------------

  redis-exporter:
    image: oliver006/redis_exporter
    container_name: redis-exporter
    restart: unless-stopped
    ports:
      - "9121:9121"
    networks: [labnet]

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=200h"
      - "--web.enable-lifecycle"
    depends_on:
      - event-ingestor
      - correlator
    networks: [labnet]

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    networks: [labnet]

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    restart: unless-stopped
    ports:
      - "8088:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    networks: [labnet]

  # ----------------------- Graficos -----------------------

  grafana:
    image: grafana/grafana:11.1.4
    container_name: ui-grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-grafana}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-grafana123}
      GF_INSTALL_PLUGINS: "redis-datasource,grafana-clock-panel"
      GF_FEATURE_TOGGLES_ENABLE: "publicDashboards"
    ports:
      - "3000:3000"
    volumes:
      - grafanadata:/var/lib/grafana
      - ./monitoring/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - postgres
      - redis
      - prometheus
    networks: [labnet]

  # ----------------------- Elasticsearch y Kibana -----------------------

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.3
    container_name: es01
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - ingest.geoip.downloader.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost:9200 || exit 1"]
      interval: 20s
      timeout: 10s
      retries: 40
      start_period: 90s
    networks: [labnet]

  elasticsearch-exporter:
    image: quay.io/prometheuscommunity/elasticsearch-exporter:latest
    container_name: elasticsearch-exporter
    restart: unless-stopped
    ports:
      - "9114:9114"
    command:
      - "--es.uri=http://es01:9200"
    depends_on:
      es01:
        condition: service_healthy
    networks: [labnet]

  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.3
    container_name: kibana
    restart: unless-stopped
    environment:
      - ELASTICSEARCH_HOSTS=http://es01:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD:-kibana123}
      - SERVER_PUBLICBASEURL=http://localhost:5601
      - XPACK_SECURITY_ENCRYPTIONKEY=ThisIsA32bytesMinKey_____1
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=ThisIsA32bytesMinKey_____2
      - XPACK_REPORTING_ENCRYPTIONKEY=ThisIsA32bytesMinKey_____3
    ports:
      - "5601:5601"
    depends_on:
      es01:
        condition: service_started
    networks: [labnet]

  # ----------------------- Airflow -----------------------

  airflow-webserver:
    image: apache/airflow:2.10.2
    container_name: airflow-web
    restart: unless-stopped
    command: webserver
    ports:
      - "8081:8080"
    environment:
      AIRFLOW_CORE_LOAD_EXAMPLES: "false"
      AIRFLOW_CORE_EXECUTOR: CeleryExecutor
      AIRFLOW_DATABASE_SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW_CELERY_RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW_CELERY_BROKER_URL: redis://cache-redis:6379/1
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
      airflow-scheduler:
        condition: service_started
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/logs:/opt/airflow/logs
    networks: [labnet]

  airflow-scheduler:
    image: apache/airflow:2.10.2
    container_name: airflow-scheduler
    restart: unless-stopped
    command: scheduler
    environment:
      AIRFLOW_CORE_LOAD_EXAMPLES: "false"
      AIRFLOW_CORE_EXECUTOR: CeleryExecutor
      AIRFLOW_DATABASE_SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW_CELERY_RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW_CELERY_BROKER_URL: redis://cache-redis:6379/1
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/logs:/opt/airflow/logs
    networks: [labnet]

  airflow-triggerer:
    image: apache/airflow:2.10.2
    container_name: airflow-triggerer
    restart: unless-stopped
    command: triggerer
    environment:
      AIRFLOW_DATABASE_SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    networks: [labnet]

  airflow-worker:
    image: apache/airflow:2.10.2
    container_name: airflow-worker
    restart: unless-stopped
    command: celery worker
    environment:
      AIRFLOW_CORE_EXECUTOR: CeleryExecutor
      AIRFLOW_DATABASE_SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW_CELERY_RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW_CELERY_BROKER_URL: redis://cache-redis:6379/1
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
      airflow-scheduler:
        condition: service_started
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    networks: [labnet]

  airflow-flower:
    image: apache/airflow:2.10.2
    container_name: airflow-flower
    restart: unless-stopped
    command: celery flower
    ports:
      - "5555:5555"
    environment:
      AIRFLOW_DATABASE_SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW_CELERY_BROKER_URL: redis://cache-redis:6379/1
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    networks: [labnet]

  # ----------------------- Aplicaciones Personalizadas -----------------------

  event-ingestor:
    build:
      context: ./EventIngestor
      dockerfile: Dockerfile
    container_name: event-ingestor
    ports:
      - "5245:5245"
    environment:
      - ASPNETCORE_ENVIRONMENT=Development
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks: [labnet]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5245/events/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  correlator:
    build:
      context: ./Correlator
      dockerfile: Dockerfile
    container_name: correlator
    ports:
      - "5246:5246"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
    environment:
      ASPNETCORE_ENVIRONMENT: "Production"
    networks: [labnet]
    restart: always

volumes:
  pgdata:
  redisdata:
  kafkadata:
  redisinsight:
  grafanadata:
  esdata:
  prometheus_data:

networks:
  labnet: {}
