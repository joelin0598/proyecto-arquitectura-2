version: "3.9"

name: monitoreo-stack

  # ----------------------- Postgres y PgAdmin -----------------------
services:
  postgres:
    image: postgres:16-alpine
    container_name: db-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-appuser}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-appsecret}
      POSTGRES_DB: ${POSTGRES_DB:-alertsdb}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [labnet]   # <<< agregado

  pgadmin:
    image: dpage/pgadmin4:8
    container_name: ui-pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-admin@example.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD:-admin123}
    ports:
      - "5050:80"
    depends_on:
      postgres:
        condition: service_healthy
    networks: [labnet]   # <<< agregado
  
  # ----------------------- Kafka y Kafka UI -----------------------

  kafka:
    image: confluentinc/cp-kafka:7.7.0
    container_name: kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      # --- KRaft single-node ---
      CLUSTER_ID: "MkU3OEVBNTcwNTJENDM2Qk"   # <-- requerido por Confluent (22 chars base64-like)
      KAFKA_NODE_ID: "1"
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_LISTENERS: "PLAINTEXT://:9092,CONTROLLER://:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      # --- ajustes Ãºtiles ---
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: "24"
      KAFKA_NUM_PARTITIONS: "3"
      
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"

    volumes:
      - kafkadata:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "/usr/bin/kafka-topics --bootstrap-server localhost:9092 --list >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 20
    networks: [labnet]

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: unless-stopped
    ports:
      - "8085:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_READONLY: "false"
    depends_on:
      - kafka
    networks: [labnet]
  # ----------------------- Redis y RedisInsight -----------------------

  redis:
    image: redis:7-alpine
    container_name: cache-redis
    restart: unless-stopped
    command: ["redis-server", "--appendonly", "yes", "--save", "60", "1000"]
    ports:
      - "6379:6379"
    volumes:
      - redisdata:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [labnet]   # <<< agregado

  redisinsight:
    image: redis/redisinsight:latest
    container_name: ui-redisinsight
    restart: unless-stopped
    ports:
      - "5540:5540"
    volumes:
      - redisinsight:/data
    depends_on:
      redis:
        condition: service_started
    networks: [labnet]   # <<< agregado

  # ----------------------- Graficos-----------------------

  grafana:
    image: grafana/grafana:11.1.4
    container_name: ui-grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-grafana}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-grafana123}
      GF_INSTALL_PLUGINS: "redis-datasource"
    ports:
      - "3000:3000"
    volumes:
      - grafanadata:/var/lib/grafana
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    networks: [labnet]   # <<< agregado
  
  # ----------------------- Elasticsearch y Kibana -----------------------

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.3
    container_name: es01
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD:-elastic123}
      - KIBANA_PASSWORD=${KIBANA_PASSWORD:-kibana123}
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - ingest.geoip.downloader.enabled=false
      - xpack.security.enrollment.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9200 >/dev/null 2>&1 || curl -fsS http://localhost:9200 >/dev/null 2>&1 || exit 1"]
      interval: 20s
      timeout: 10s
      retries: 40
      start_period: 90s
    networks: [labnet]   # <<< agregado

  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.3
    container_name: kibana
    restart: unless-stopped
    environment:
      - ELASTICSEARCH_HOSTS=http://es01:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD:-kibana123}
      - SERVER_PUBLICBASEURL=http://localhost:5601
      - XPACK_SECURITY_ENCRYPTIONKEY=ThisIsA32bytesMinKey___________1
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=ThisIsA32bytesMinKey___________2
      - XPACK_REPORTING_ENCRYPTIONKEY=ThisIsA32bytesMinKey___________3
    ports:
      - "5601:5601"
    depends_on:
      es01:
        condition: service_started
    networks: [labnet]   # <<< agregado

  # ----------------------- Airflow (usando postgres y redis existentes) -----------------------
  airflow-webserver:
    image: apache/airflow:2.10.2
    container_name: airflow-web
    restart: unless-stopped
    command: webserver
    ports:
      - "8080:8080"
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://cache-redis:6379/1
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy     # <<< ajustado
      redis:
        condition: service_started     # <<< ajustado
      airflow-scheduler:
        condition: service_started
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/logs:/opt/airflow/logs
    networks: [labnet]

  airflow-scheduler:
    image: apache/airflow:2.10.2
    container_name: airflow-scheduler
    restart: unless-stopped
    command: scheduler
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://cache-redis:6379/1
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy     # <<< ajustado
      redis:
        condition: service_started     # <<< ajustado
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/logs:/opt/airflow/logs
    networks: [labnet]

  airflow-triggerer:
    image: apache/airflow:2.10.2
    container_name: airflow-triggerer
    restart: unless-stopped
    command: triggerer
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy     # <<< ajustado
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    networks: [labnet]

  airflow-worker:
    image: apache/airflow:2.10.2
    container_name: airflow-worker
    restart: unless-stopped
    command: celery worker
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://cache-redis:6379/1
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy     # <<< ajustado
      redis:
        condition: service_started     # <<< ajustado
      airflow-scheduler:
        condition: service_started
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    networks: [labnet]

  airflow-flower:
    image: apache/airflow:2.10.2
    container_name: airflow-flower
    restart: unless-stopped
    command: celery flower
    ports:
      - "5555:5555"
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://cache-redis:6379/1
      _PIP_ADDITIONAL_REQUIREMENTS: >
        apache-airflow-providers-apache-kafka==1.10.4
        confluent-kafka==2.11.1
    depends_on:
      postgres:
        condition: service_healthy     # <<< ajustado
      redis:
        condition: service_started     # <<< ajustado
    networks: [labnet]
  event-ingestor:
    build:
      context: ./EventIngestor
      dockerfile: Dockerfile
    container_name: event-ingestor
    ports:
      - "5245:5245"
    environment:
      - ASPNETCORE_ENVIRONMENT=Development
    depends_on:
      postgres:
        condition: service_healthy     # <<< ajustado
      redis:
        condition: service_healthy     # <<< ajustado
    networks: [labnet]
    restart: unless-stopped
volumes:
  pgdata:
  redisdata:
  kafkadata:
  redisinsight:
  grafanadata:
  esdata:

networks:
  labnet: {}      # <<< agregado

